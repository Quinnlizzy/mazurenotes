Binary classification

Classification, like regression is SML technique.
Therefore follows same iterative process; training, validating, evaluating models. 

        BUT Instead of calculating numeric values like regression;
            - algorithms used to train classification models calculate probability values for class assignment 
            - & evaluation metrics that assess model performance compare predicted classes to actual classes.

BinClass algorithms used to train model that predicts one of two possible labels for single class
            - Basically; true or false. 
            - Most scenarios; data obs used to train & validate model have multiple feature values(x) and a (y) value = 1 or 0.



Example - binClass
Simplified example using single feature (x) to predict whether label (y) is 1 or 0. 
Use blood glucose level of a patient to predict if has diabetes. Data:

            Blood glucose (x)	Diabetic? (y)
                            67	0
                            103	1
                            114	1
                            72	0
                            116	1
                            65	0


Training binClass model
Use an algorithm to fit training data to function that calculates probability of class label being true (diabetic)
Probability measured as value between 0.0 and 1.0
Total probability for all possible classes = 1.0. 

            E.g. if probability of diabetes = 0.7, then corresponding probability of 0.3 of no diabetes.

Many algorithms can be used for binClass.



Logistic regression algorithm;

Derives sigmoid (S-shaped graph) function with values between 0.0 and 1.0.
Function produced by algorithm describes probability of y being true (y=1) for given value of x. 
Mathematically, express function like this: f(x) = P(y=1 | x)

            For 3.6 obs in training data, we know y is true.
            Therefore, probability for those obs that y=1 is 1.0
            For other three, we know y is false.
            Therefore, probability that y=1 is 0.0. 
            S-shaped curve describes probability distribution; 
            Therefore, plotting a value of x on the line identifies corresponding probability that y is 1.

Model diagram also includes horizontal line. 
This indicates threshold where model based on this func will predict true(1)/false(0).
Threshold lies at mid-point for y (P(y) = 0.5). 
Values at this point or above; model predicts true (1);
Values below will predict false (0). 

            E.g. patient with blood glucose 90; func result in probability value 0.9. 
            0.9 higher than threshold of 0.5 so model predicts true (1) (diabetes).



Evaluating binClass Model
Like regression, hold back random subset of data to validate trained model. 
        
        Assume held back following data to validate our diabetes classifier:
                Blood glucose (x)	Diabetic? (y)
                                66	0
                                107	1
                                112	1
                                71	0
                                87	1
                                89	1
        Applying logistic func derived previously to the x values results in new plot.
        Based on whether probability calculated by func is above/below threshold,
        the model generates predicted label of 1 or 0 for each obs. 
        We then compare predicted class labels (ŷ) to actual class labels (y):

                Blood glucose (x)	Actual diabetes diagnosis (y)	Predicted diabetes diagnosis (ŷ)
                                                66	0	0
                                                107	1	1
                                                112	1	1
                                                71	0	0
                                                87	1	0
                                                89	1	1
    


BinClass evaluation metrics

First step in calc eval metrics for binClass model is to create matrix of no. of correct/incorrect predictions for each 
possible class label:

       Confusion matrix. Shows the prediction totals where:

                            ŷ=0 and y=0: True negatives (TN)
                            ŷ=1 and y=0: False positives (FP)
                            ŷ=0 and y=1: False negatives (FN)
                            ŷ=1 and y=1: True positives (TP)

       The arrangement of confusion matrix shows that correct (true) predictions are shown in a diagonal line from 
       top-left to bottom-right. 
       Often, color-intensity is used to indicate no. of predictions in each cell, 
       (so a quick glance at a model that predicts well should reveal deeply shaded diagonal trend).



Accuracy
Simplest metric calculable from confusion matrix is accuracy (proportion of predictions that the model got right).
        Accuracy is calculated as:

                                    (TN+TP) ÷ (TN+FN+FP+TP)

        In the case of our diabetes example, the calculation is:

                                    (2+3) ÷ (2+1+0+3)
                                    = 5 ÷ 6
                                    = 0.83

        So model produced correct predictions 83% of the time.

Accuracy might initially seem a good metric to eval a model
BUT suppose 11% of population has diabetes; 
                  in theory you could create a model that just always predicts 0, 
                  and it would achieve an accuracy of 89% (even though it makes attempt to differentiate)
                  Really need deeper understanding of how model performs at predicting 1 for +ve & 0 -ve.



Recall
A metric that measures proportion of +ve cases that the model identified correctly.
Compare number of patients who have ACTUALLY have diabetes, to how many model predicted have diabetes?

                    The formula for recall is: TP ÷ (TP+FN)
                    For our diabetes example:  3 ÷ (3+1)
                                               = 3 ÷ 4
                                               = 0.75

So model correctly identified 75% of patients who ACTUALLY have diabetes as having diabetes.



Precision
Similar to recall, but measures proportion of predicted +ve cases where true label is actually positive. 
Basically, what proportion of patients the model predicted to have diabetes ACTUALLY have diabetes?

                    The formula for precision is: TP ÷ (TP+FP)
                    For our diabetes example:     3 ÷ (3+0)
                                                  = 3 ÷ 3
                                                  = 1.0

So 100% of patients predicted by model to have diabetes ACTUALLY have diabetes.



F1-score
An overall metric that combines recall & precision. 

                    Formula for F1-score is:   (2 x Precision x Recall) ÷ (Precision + Recall)
                    For our diabetes example:  (2 x 1.0 x 0.75) ÷ (1.0 + 0.75)
                                               = 1.5 ÷ 1.75
                                               =0.86



Area Under the Curve (AUC)
Another name for recall is true positive rate (TPR)
equivalent metric called the false positive rate (FPR) that is calculated as FP÷(FP+TN). 
Already know that the TPR for our model when using a threshold of 0.5 is 0.75.
Can use the formula for FPR to calculate a value of 0÷2 = 0.

        If we change threshold for true (1), it would affect no. of +ve & -ve predictions & change the TPR and FPR metrics. 
        These metrics are used to evaluate a model by plotting a received operator characteristic (ROC) curve 
        (compares the TPR and FPR for every possible threshold value between 0.0 and 1.0)


ROC curve for perfect model would go straight up TPR axis on left and across the FPR axis top. 
Since plot area for curve measures 1x1, the area under this perfect curve would be 1.0 (means model correct 100% of time). 

A diagonal line from bottom-left to top-right = results that would be achieved by randomly guessing a binary label;
producing an area under the curve of 0.5.
In other words, given two possible class labels, expect to guess correctly 50% of time.

        In case of diabetes model, the curve above is produced & area under curve (AUC) metric is 0.875. 
        Since AUC higher than 0.5, we can conclude model performs better at predicting if patient diabetes than randomly guessing.