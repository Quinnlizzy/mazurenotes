Regression

these models are trained to predict numeric label values
they are based on training data that includes both features and known labels

Process for training a regression model:
           - multiple iterations in which you use an appropriate algorithm (usually with some parameterized settings)
           - to train model,
           - evaluate model predictive performance
           - refine model by repeating training process with different algorithms/parameters
           - until achieve acceptable level of predictive accuracy.

4 key elements of SML training models:

         1 - Split training data (randomly) to create dataset to train model + holding back subset of data for model validation.
         2 - Use algorithm to fit training data to model. In regression model, use regression algorithm (e.g. linear regression).
         3 - Use held back validation data from step 1 to test model by predicting labels for features.
         4 - Compare known actual labels in validation dataset to the labels that the model predicted. 
             Then aggregate differences between predicted/actual label values 
             Then calculate metric that indicates model prediction accuracy for validation data.

    After each train/validate/evaluate iteration, repeat process with different algorithms/parameters 
    do until acceptable evaluation metric is achieved.


Example:

Train a model to predict numeric label (y) based on single feature value (x). 
(Most real scenarios involve multiple feature values, more complex but the principle is the same)

Based on ice cream sales scenario;
         For our feature, consider temperature (max temp of a day), 
         For label, we want to train model to predict the number of ice creams sold that day. 
         
         We start with historic data that includes records of daily tempe (x) and ice cream sales (y):

         Temperature (x)	Ice cream sales (y)
                    51	     1
                    52	     0
                    67	     14
                    65       14
                    70	     23
                    69	     20
                    72	     23
                    75	     26
                    73	     22
                    81	     30
                    78	     26
                    83	     36

        Then we train the model.
        Start by splitting data & using subset of it to train a model. 
        Here's the training dataset:

         Temperature (x)	Ice cream sales (y)
                    51	     1
                    65	     14
                    69	     20
                    72	     23
                    75	     26
                    81	     30
    
        Then, to understand how relate to one another - plot cordinates along two axes. Point graph.

        Now can apply algorithm to training data & fit to a function that applies operation to (x) to calculate y. 
        
        One algorithm is linear regression; works by deriving a function that produces a straight line through the intersections 
        of x and y values while minimizing the average distance between line and plotted points
        (think line showing general trend on a graph)

        Line is a visual representation of the function where slope the line shows how calculate value of y for given value of x. 
        E.g. Line intercepts the x axis at 50, so when x = 50, y = 0. 
        Line slopes so that every increase of 5 along the x axis = increase of 5 up y axis (x=55, y=5; x=60, y=10)
       

                 So, to calculate value of y when given value of x, the function simply subtracts 50; f(x) = x-50


        Can now predict no. of ice creams sold on a day by its temp. 
        E.g. if weather will be 77 degrees; 77-50 = sell 27 ice creams.


Evaluating regression model

        How accurate is model?
        To validate model and evaluate prediction quality we need the held back dataset:

        Temperature (x)	Ice cream sales (y)
                    52	0
                    67	14
                    70	23
                    73	22
                    78	26
                    83	36

            Now, use our model to predict the label for each of the obs in this dataset.
            So, based on (x) value; compare predicted label (ŷ)(so x-50) to known actual label value (y)(what happened in reality).

            Using model trained earlier, (function f(x) = x-50) results in the following predictions:

            Temperature (x)	Actual sales (y)	Predicted sales (ŷ)
                    52	           0                 	2
                    67	           14                	17
                    70	           23	                20
                    73	           22	                23
                    78	           26	                28
                    83	           36	                33

            
            Now again plot on axes labels(pred and act) against feature values.

            Predicted labels are calculated by the model so they're on the 'function line'
            Shows there's variance between the ŷ(predicted) and y(actual) values from the validation dataset. 
            Can show this on plot as line between ŷ and y values (shows how far off prediction was).


Regression evaluation metrics

        Based on diffs between predicted & actual values, can calculate common metrics used to evaluate regression model.


Mean Absolute Error (MAE)

        Variance in our example indicates by how many ice creams each prediction was wrong. 
        Doesn't matter if prediction was over/under (-3 & +3 both indicate variance of 3). 
        This metric is known as 'absolute error' for each prediction.
        Summarized for whole validation set as mean absolute error (MAE).
                Ice cream example; the mean (average) of the absolute errors (2, 3, 3, 1, 2, and 3) is 2.33.


Mean Squared Error (MSE)  

        Mean absolute error metric takes all discrepancies between predicted & actual labels into account equally.
        However, may be better to have model that is consistently wrong by a SMALL amount instead of fewer, but larger errors.
        Can produce metric that "amplifies" larger errors by squaring individual errors and calculating mean of squared values.
        This is known as mean squared error (MSE).
                Ice cream example; mean of squared absolute values (4, 9, 9, 1, 4, and 9) is 6.


Root Mean Squared Error (RMSE)

        Mean squared error helps take magnitude of errors into account
        But because it squares error values, resulting metric no longer represents the quantity measured by label.
        Can say MSE of our model = 6, but that doesn't measure its accuracy in terms of no. of ice creams mispredicted
        (6 is just numeric score that indicates level of error in validation predictions).
        To measure error in tetms of no. of ice creams, need to calculate the square root of MSE.
        This produces metric called Root Mean Squared Error. So √6 is 2.45 (ice creams).


Coefficient of determination (R2)

        All metrics so far evaluate model by compare discrepancy between the predicted and actual values.
        However, in reality, there's natural random variance in daily sales of ice cream the model takes into account.
        In linear reg model; training algorithm fits straight line that minimizes mean variance between func & known label.
            Coefficient of determination (R2/R-Squared) is metric that measures proportion of variance in validation results 
            that can be explained by model, as opposed to any anomalous aspect of validation data 
            (e.g. a day with unusual no. of ice creams sales because local event).

        The calculation for R2 is more complex than previous metrics. 
        It compares:
            a)sum of squared differences between predicted & actual labels
            b)with sum of squared differences between the actual label values & mean of actual label values:

            R2 = 1- ∑(y-ŷ)2 ÷ ∑(y-ȳ)2

        Most machine learning tools can calculate this metric for you. 
        Important point is the result = value between 0 and 1 that describes proportion of variance explained by the model.
            
            The closer to 1 R2 is = better the model is fitting the validation data. 
            In ice cream regression model, the R2 = 0.95.


Iterative training
        Metrics described above = commonly used to evaluate reg model. 
        In most real-world scenarios, data scientist will use an iterative process
        This will repeatedly train and evaluate a model.
        They will vary:

            - Feature selection and preparation 
              (choosing which features to include in model & calculations applied to them to help ensure a better fit).
            - Algorithm selection 
              (far more than just linear regression algorithms)
            - Algorithm parameters
              (numeric settings to control algorithm behavior, called hyperparameters to differentiate from x and y parameters).
            
        After multiple iterations; model that results in the best evaluation metric acceptable for specific scenario is selected.