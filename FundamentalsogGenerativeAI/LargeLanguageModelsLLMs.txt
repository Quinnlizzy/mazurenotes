Large language models
Generative AI app are powered by large language models (LLMs), 
a specialized type of ML model that you can use to perform natural language processing (NLP) tasks:
        Determining sentiment or otherwise classifying natural language text.
        Summarizing text.
        Comparing multiple text sources for semantic similarity.
        Generating new natural language.

mathematical principles behind these LLMs can be complex, 
but basic understanding of the architecture used implement them gives conceptual understanding.


Transformer models
    ML models for NLP have evolved over many years. 
    Today's cutting-edge LLM's are based on the transformer architecture, 
    builds on & extends some techniques that have been proven successful in modeling vocabularies to support NLP tasks - 
    particularly in generating language. 
    Transformer models trained with large volumes of text
    Enables them to represent the semantic relationships between words 
    And with this use those relationships to determine probable sequences of text that make sense. 
    Transformer models with large enough vocabulary capable of genlanguage responses that are tough to distinguish from humans.


Transformer model architecture consists of two components, or blocks:
    Encoder block that creates semantic representations of training vocabulary.
    Decoder block that generates new language sequences.

In practice, specific implementations of architecture vary – 
    e.g. - the Bidirectional Encoder Representations from Transformers (BERT) model 
           developed by Google to support their search engine uses only the encoder block,
         - the Generative Pretrained Transformer (GPT) model
           developed by OpenAI uses only the decoder block.

An explanation of some key elements in transformer can help get sense of how support generative AI.



Tokenization

first step in training a transformer model is decompose the training text into tokens
Basically, identify each unique text value. 
Can think of each distinct word in training text as a token 
(in reality, tokens can be generated for partial words, or combinations of words and punctuation).
    e.g. consider the following sentence:
           I heard a dog bark loudly at a cat

    To tokenize this text, can identify each discrete word & assign token IDs. 
    e.g.    I (1)
            heard (2)
            a (3)
            dog (4)
            bark (5)
            loudly (6)
            at (7)
            ("a" is already tokenized as 3)
            cat (8)

    Sentence can now be represented with the tokens: [1 2 3 4 5 6 7 3 8]. 
    Similarly, the sentence "I heard a cat" could be represented as [1 2 3 8].

    As you continue to train model, each new token in training text is added to vocabulary with appropriate token IDs:
            meow (9)
            skateboard (10)
                            and so on...

    With sufficiently large set of training text, a vocab of thousands of tokens could be compiled.



Embeddings

While convenient to represent tokens as simple IDs - creating an index for all the words
they don't tell anything about meaning of words or the relationships between them. 
To create vocab that encapsulates semantic relationships between tokens, define contextual vectors, 'embeddings', for them. 
Vectors are multi-valued numeric representations of information, e.g. [10, 3, 1] 
in which each numeric element represents particular attribute of info. 

For language tokens, each element of token's vector reps semantic attribute of token. 
Specific categories for elements of vectors in a language model are determined during training
They are based on how commonly words are used together/in similar contexts.

Useful to think of elements in a token embedding vector as coordinates in multidimensional space
each token occupies a specific "location." 
The closer tokens are to one another along particular dimension, more semantically related they are. 
In other words, related words are grouped closer together. 

    As a simple example, suppose embeddings for tokens consist of vectors with 3 elements:
        4 ("dog"): [10,3,2]
        5 ("bark"): [10,2,2]
        8 ("cat"): [10,3,1]
        9 ("meow"): [10,2,1]
        10 ("skateboard"): [3,3,1]

    Can plot location of tokens based on these vectors in three-dimensional space

    Locations of tokens in embeddings space include some information about how closely the tokens are related
         For example - the token for "dog" is close to "cat" and also to "bark." 
                     - The tokens for "cat" and "bark" are close to "meow."     
                     - The token for "skateboard" is further away from the other tokens.

    This example shows a simple example model in which each embedding has only three dimensions. 
    Real language models have many more dimensions.
    There are multiple ways can calculate appropriate embeddings for a given set of tokens, 
    including language modeling algorithms like Word2Vec or the encoder block in a transformer model.

//

Attention

The encoder and decoder blocks in a transformer model include multiple layers that form the neural network for the model. We don't need to go into the details of all these layers, but it's useful to consider one of the types of layers that is used in both blocks: attention layers. Attention is a technique used to examine a sequence of text tokens and try to quantify the strength of the relationships between them. In particular, self-attention involves considering how other tokens around one particular token influence that token's meaning.

In an encoder block, attention is used to examine each token in context, and determine an appropriate encoding for its vector embedding. The vector values are based on the relationship between the token and other tokens with which it frequently appears. This contextualized approach means that the same word might have multiple embeddings depending on the context in which it's used - for example "the bark of a tree" means something different to "I heard a dog bark."

In a decoder block, attention layers are used to predict the next token in a sequence. For each token generated, the model has an attention layer that takes into account the sequence of tokens up to that point. The model considers which of the tokens are the most influential when considering what the next token should be. For example, given the sequence “I heard a dog,” the attention layer might assign greater weight to the tokens “heard” and “dog” when considering the next word in the sequence:

I heard a dog [bark]

Remember that the attention layer is working with numeric vector representations of the tokens, not the actual text. In a decoder, the process starts with a sequence of token embeddings representing the text to be completed. The first thing that happens is that another positional encoding layer adds a value to each embedding to indicate its position in the sequence:

[1,5,6,2] (I)
[2,9,3,1] (heard)
[3,1,1,2] (a)
[4,10,3,2] (dog)
During training, the goal is to predict the vector for the final token in the sequence based on the preceding tokens. The attention layer assigns a numeric weight to each token in the sequence so far. It uses that value to perform a calculation on the weighted vectors that produces an attention score that can be used to calculate a possible vector for the next token. In practice, a technique called multi-head attention uses different elements of the embeddings to calculate multiple attention scores. A neural network is then used to evaluate all possible tokens to determine the most probable token with which to continue the sequence. The process continues iteratively for each token in the sequence, with the output sequence so far being used regressively as the input for the next iteration – essentially building the output one token at a time.

The following animation shows a simplified representation of how this works – in reality, the calculations performed by the attention layer are more complex; but the principles can be simplified as shown:

Animation showing an attention layer assigning weights to tokens and predicting the next one.

A sequence of token embeddings is fed into the attention layer. Each token is represented as a vector of numeric values.
The goal in a decoder is to predict the next token in the sequence, which will also be a vector that aligns to an embedding in the model’s vocabulary.
The attention layer evaluates the sequence so far and assigns weights to each token to represent their relative influence on the next token.
The weights can be used to compute a new vector for the next token with an attention score. Multi-head attention uses different elements in the embeddings to calculate multiple alternative tokens.
A fully connected neural network uses the scores in the calculated vectors to predict the most probable token from the entire vocabulary.
The predicted output is appended to the sequence so far, which is used as the input for the next iteration.
During training, the actual sequence of tokens is known – we just mask the ones that come later in the sequence than the token position currently being considered. As in any neural network, the predicted value for the token vector is compared to the actual value of the next vector in the sequence, and the loss is calculated. The weights are then incrementally adjusted to reduce the loss and improve the model. When used for inferencing (predicting a new sequence of tokens), the trained attention layer applies weights that predict the most probable token in the model’s vocabulary that is semantically aligned to the sequence so far.

What all of this means, is that a transformer model such as GPT-4 (the model behind ChatGPT and Bing) is designed to take in a text input (called a prompt) and generate a syntactically correct output (called a completion). In effect, the “magic” of the model is that it has the ability to string a coherent sentence together. This ability doesn't imply any “knowledge” or “intelligence” on the part of the model; just a large vocabulary and the ability to generate meaningful sequences of words. What makes a large language model like GPT-4 so powerful however, is the sheer volume of data with which it has been trained (public and licensed data from the Internet) and the complexity of the network. This enables the model to generate completions that are based on the relationships between words in the vocabulary on which the model was trained; often generating output that is indistinguishable from a human response to the same prompt.